---
title: "Introduction to dblinkR"
author: "Neil Marchant"
date: "18 March 2020"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to dblinkR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width=7, 
  fig.height=5
)
```

We begin by loading the required packages. Note that the `dblinkR` package 
must be loaded *after* `sparklyr`.
```{r, echo=FALSE}
# For running dblink
library(sparklyr)
library(dblinkR)

# For generating trace plots
library(stringr)
library(dplyr)
library(tidyr)
library(ggplot2)
```

## Connecting to Spark
In this vignette, we'll run `dblink` on a local instance of Spark with 2 
cores. This is sufficient for testing purposes, however for larger data sets, 
we recommend connecting to a Spark deployment. The `sparklyr` 
[documentation](https://spark.rstudio.com/deployment/) contains information 
about connecting to Spark deployments.
```{r, echo=FALSE}
sc <- spark_connect(master = "local[2]", version = "2.4.3")
spark_context(sc) %>% invoke("setLogLevel", "WARN")
```

`dblink` requires a location on disk (includes HDFS) to save diagnostics, 
posterior samples and the state of the Markov chain. We refer to this location 
as the *project path*, and set it to the current working directory below. 
We must also specify a location on disk to save Spark checkpoints. 
```{r, echo=FALSE}
projectPath <- paste0(getwd(), "/") # working directory
spark_set_checkpoint_dir(sc, paste0(projectPath, "checkpoints/"))
```

## Synthetic data
We'll experiment with the `RLdata500` synthetic data included with the package. 
It contains 500 records, 50 of which are duplicates with random distortions. 
We add in a column for record identifiers, which will be used to specify 
the linkage structure.
```{r}
records <- RLdata500
records['rec_id'] <- seq_len(nrow(records))
records <- lapply(records, as.character) %>% as.data.frame(stringsAsFactors = FALSE)
head(RLdata500)
```

## dblink parameters
Next we specify the parameters for the `dblink` model. We treat the 
name-related attributes as string-type with a Levenshtein similarity 
function, and the date-related attributes as categorical (with a constant 
similarity function). We also place a Beta prior on the distortion probability 
for each attribute.
```{r}
distortionPrior <- BetaRV(0.5, 50)
levSim <- LevenshteinSimFn(threshold = 7, maxSimilarity =  10)

attributeSpecs <- list(
  fname_c1 = Attribute(levSim, distortionPrior),
  lname_c1 = Attribute(levSim, distortionPrior),
  by = CategoricalAttribute(distortionPrior),
  bm = CategoricalAttribute(distortionPrior),
  bd = CategoricalAttribute(distortionPrior)
)
```

The Beta prior on the distortion probabilities favours low distortion, as 
shown below.
```{r}
tibble(prob = seq(from=0, to=1, by=0.01)) %>%
  mutate(density = dbeta(prob, distortionPrior@shape1, distortionPrior@shape2)) %>%
  ggplot(aes(x = prob, y = density)) + geom_line() + 
  labs(title = "Prior distribution on the distortion probabilities", 
       x = "Distortion probability", y = "Density")
```

We can also specify the size of the latent entity population. A larger 
population size favours under-linkage, while a smaller population 
size favours over-linkge. By default, the population size is set to the 
number of records in the data. This is specified by setting the population 
size to `NULL`:
```{r}
populationSize <- NULL
```

Finally, we specify the partitioner. Since the data is small in this case, 
we can switch partitioning off by setting the partitioner to `NULL`. 
For larger data sets, we recommend using the `KDTreePartitioner`.
```{r}
partitioner <- NULL
```

## Running inference
Now that we've specified the model parameters, we're ready to run inference. 
The results will be saved in the project path.

```{r}
state <- initializeState(sc, records, attributeSpecs, recIdColname = 'rec_id',
                         partitioner = partitioner, populationSize = populationSize,
                         randomSeed = 1, maxClusterSize = 10L)

result <- runInference(state, projectPath, sampleSize = 100, 
                       burninInterval = 1000, thinningInterval = 10)
```

To load previously saved results, we can use the following function.
```{r, eval=FALSE}
# result <- loadResult(sc, projectPath)
```


## MCMC diagnostics
Diagnostic statistics along the Markov chain are saved in the 
`diagnostics.csv` file located in the project path. We can use the function 
below to load these into a local tibble.
```{r}
diagnostics <- loadDiagnostics(sc, projectPath)
```

It's useful to examine trace plots, to assess the convergence.
```{r}
ggplot(diagnostics, aes(x=iteration, y=numObservedEntities)) + 
  geom_line() + 
  labs(title = 'Trace plot: # entity clusters', x = 'Iteration', 
       y = '# clusters')

diagnostics %>%
  select(iteration, starts_with("aggDist")) %>%
  gather(attribute, numDistortions, starts_with("aggDist")) %>%
  mutate(attribute = str_match(attribute, "^aggDist(.+)")[,2],
         numDistortions = numDistortions / nrow(records)) %>% 
  ggplot(aes(x=iteration, y=numDistortions)) +  
  geom_line(aes(colour = attribute)) + 
  labs(title = 'Trace plot: attribute distortion', x = 'Iteration', 
       y = '% distorted', colour = 'Attribute')

ggplot(diagnostics, aes(x=iteration, y=logLikelihood)) + 
  geom_line() + 
  labs(title = 'Trace plot: log-likelihood (unnormalized)', x = 'Iteration', 
       y = 'log-likelihood')
```

Additional diagnostics statistics can be computed from the samples of the 
linkage structure, which we refer to as the *linkage chain*. 
```{r}
linkageChain <- loadLinkageChain(sc, projectPath)

clustSizeDist <- clusterSizeDistribution(linkageChain)
ggplot(clustSizeDist, aes(x=iteration, y=frequency)) + 
  geom_line(aes(colour = clusterSize)) + 
  labs(title = 'Trace plot: cluster size distribution', x = 'Iteration', 
       y = 'Frequency', colour = 'Cluster size')

partSizes <- partitionSizes(linkageChain)
ggplot(partSizes, aes(x=iteration, y=size)) + 
  geom_line(aes(colour = partitionId)) + 
  labs(title = 'Trace plot: partition sizes', x = 'Iteration', y = 'Size', 
       colour = 'Partition')
```

## Evaluation
We compute a point estimate of the posterior linkage structure/clustering 
using the *shared most probable maximal matching sets method*. Note that we 
must use the `collect` function to retrieve the data from Spark into a local 
tibble.
```{r}
predClusters <- dblinkR::sharedMostProbableClusters(linkageChain) %>% collect()
```

Next we assess the quality of the predicted linkage structure by 
computing pairwise precision and recall using functions from the 
[`exchangeableER`](https://github.com/ngmarchant/exchangeableER) package.
```{r}
library(exchangeableER)
trueClusters <- exchangeableER::membershipToClusters(identity.RLdata500, ids = records$rec_id)
predMatches <- exchangeableER::clustersToPairs.Clusters(predClusters)
trueMatches <- exchangeableER::clustersToPairs.Clusters(trueClusters)
numRecords <- nrow(records)
conMat <- exchangeableER::confusionMatrix(predMatches, trueMatches, numRecords*(numRecords - 1)/2)
print(exchangeableER::pairwiseMetrics(conMat))
```




