---
title: "Introduction to dblinkR"
author: "Neil Marchant and Rebecca C. Steorts"
date: "18 March 2020"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to dblinkR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width=7, 
  fig.height=5
)
```

We begin by loading the required packages. Note that the `dblinkR` package 
must be loaded *after* `sparklyr`.
```{r packages, message=FALSE}
# For running dblink
library(sparklyr)
library(dblinkR)

# For generating trace plots
library(stringr)
library(dplyr)
library(tidyr)
library(ggplot2)

# For generating credible interval plots
library(tidybayes)
```

## Connecting to Spark
In this vignette, we'll run `dblink` on a local instance of Spark with 2 
cores. This is sufficient for testing purposes, however for larger data sets, 
we recommend connecting to a Spark deployment. The `sparklyr` 
[documentation](https://spark.rstudio.com/deployment/) contains information 
about connecting to Spark deployments.
```{r spark-connect, message=FALSE}
sc <- spark_connect(master = "local[2]", version = "2.4.3")
spark_context(sc) %>% invoke("setLogLevel", "WARN")
```

`dblink` requires a location on disk (includes HDFS) to save diagnostics, 
posterior samples and the state of the Markov chain. We refer to this location 
as the *project path*, and set it to the current working directory below. 
We must also specify a location on disk to save Spark checkpoints. 
```{r set-paths, message=FALSE}
projectPath <- paste0(getwd(), "/") # working directory
spark_set_checkpoint_dir(sc, paste0(projectPath, "checkpoints/"))
```

## Synthetic data
We'll experiment with the `RLdata500` synthetic data included with the package. 
It contains 500 records, 50 of which are duplicates with random distortions. 
We add in a column for record identifiers, which will be used to specify 
the linkage structure.
```{r load-data}
records <- RLdata500
records['rec_id'] <- seq_len(nrow(records))
records <- lapply(records, as.character) %>% as.data.frame(stringsAsFactors = FALSE)
head(RLdata500)
```

## dblink parameters
Next we specify the parameters for the `dblink` model. We treat the 
name-related attributes as string-type with a Levenshtein similarity 
function, and the date-related attributes as categorical (with a constant 
similarity function). We also place a Beta prior on the distortion probability 
for each attribute.
```{r set-attribute-parameters}
distortionPrior <- BetaRV(1, 50)
levSim <- LevenshteinSimFn(threshold = 7, maxSimilarity =  10)

attributeSpecs <- list(
  fname_c1 = Attribute(levSim, distortionPrior),
  lname_c1 = Attribute(levSim, distortionPrior),
  by = CategoricalAttribute(distortionPrior),
  bm = CategoricalAttribute(distortionPrior),
  bd = CategoricalAttribute(distortionPrior)
)
```

The Beta prior on the distortion probabilities favours low distortion, as 
shown below.
```{r plot-distortion-prior}
tibble(prob = seq(from=0, to=1, by=0.01)) %>%
  mutate(density = dbeta(prob, distortionPrior@shape1, distortionPrior@shape2)) %>%
  ggplot(aes(x = prob, y = density)) + geom_line() + 
  labs(title = "Prior distribution on the distortion probabilities", 
       x = "Distortion probability", y = "Density")
```

We can also specify the size of the latent entity population. A larger 
population size favours under-linkage, while a smaller population 
size favours over-linkge. By default, the population size is set to the 
number of records in the data. This is specified by setting the population 
size to `NULL`:
```{r set-population-size}
populationSize <- NULL
```

Finally, we specify the partitioner. Since the data is small in this case, 
we can switch partitioning off by setting the partitioner to `NULL`. 
For larger data sets, we recommend using the `KDTreePartitioner`.
```{r set-partitioner}
partitioner <- NULL
```

## Running inference
Now that we've specified the model parameters, we're ready to run inference. 
The results will be saved in the project path.

```{r run-inference}
state <- initializeState(sc, records, attributeSpecs, recIdColname = 'rec_id',
                         partitioner = partitioner, populationSize = populationSize,
                         randomSeed = 1, maxClusterSize = 10L)
```

To load previously saved results, we can use the following function.
```{r, eval=FALSE}
# result <- loadResult(sc, projectPath)
```


## MCMC diagnostics
Diagnostic statistics along the Markov chain are saved in the 
`diagnostics.csv` file located in the project path. We can use the function 
below to load these into a local tibble.
```{r read-diagnostics}
diagnostics <- loadDiagnostics(sc, projectPath)
```

It's useful to examine trace plots, to assess the convergence.
```{r diagnostic-plots-1}
ggplot(diagnostics, aes(x=iteration, y=numObservedEntities)) + 
  geom_line() + 
  labs(title = 'Trace plot: # entity clusters', x = 'Iteration', 
       y = '# clusters')

diagnostics %>%
  select(iteration, starts_with("aggDist")) %>%
  gather(attribute, numDistortions, starts_with("aggDist")) %>%
  mutate(attribute = str_match(attribute, "^aggDist(.+)")[,2],
         numDistortions = numDistortions / nrow(records)) %>% 
  ggplot(aes(x=iteration, y=numDistortions)) +  
  geom_line(aes(colour = attribute)) + 
  labs(title = 'Trace plot: attribute distortion', x = 'Iteration', 
       y = '% distorted', colour = 'Attribute')

ggplot(diagnostics, aes(x=iteration, y=logLikelihood)) + 
  geom_line() + 
  labs(title = 'Trace plot: log-likelihood (unnormalized)', x = 'Iteration', 
       y = 'log-likelihood')
```

Additional diagnostics statistics can be computed from the samples of the 
linkage structure, which we refer to as the *linkage chain*. 
```{r diagnostic-plots-2}
linkageChain <- loadLinkageChain(sc, projectPath)

clustSizeDist <- clusterSizeDistribution(linkageChain)
ggplot(clustSizeDist, aes(x=iteration, y=frequency)) + 
  geom_line(aes(colour = clusterSize)) + 
  labs(title = 'Trace plot: cluster size distribution', x = 'Iteration', 
       y = 'Frequency', colour = 'Cluster size')

partSizes <- partitionSizes(linkageChain)
ggplot(partSizes, aes(x=iteration, y=size)) + 
  geom_line(aes(colour = partitionId)) + 
  labs(title = 'Trace plot: partition sizes', x = 'Iteration', y = 'Size', 
       colour = 'Partition')
```

## Evaluation
### Linkage quality
We compute a point estimate of the posterior linkage structure/clustering 
using the *shared most probable maximal matching sets method*. Note that we 
must use the `collect` function to retrieve the data from Spark into a local 
tibble.
```{r smpc}
predClusters <- dblinkR::sharedMostProbableClusters(linkageChain) %>% collect()
```

Next we assess the quality of the predicted linkage structure by 
computing pairwise precision and recall using functions from the 
[`exchangeableER`](https://github.com/ngmarchant/exchangeableER) package.
```{r pairwise-metrics, message=FALSE}
library(exchangeableER)
trueClusters <- exchangeableER::membershipToClusters(identity.RLdata500, ids = records$rec_id)
predMatches <- exchangeableER::clustersToPairs.Clusters(predClusters)
trueMatches <- exchangeableER::clustersToPairs.Clusters(trueClusters)
numRecords <- nrow(records)
conMat <- exchangeableER::confusionMatrix(predMatches, trueMatches, numRecords*(numRecords - 1)/2)
print(exchangeableER::pairwiseMetrics(conMat))
```

### Posterior estimates

First, we compute the posterior mean, posterior variance, and a 95 percent credible interval from our resulting proposed methodology. These summary statistics is stored in the diagnostics, which we utilize below in order to calculate our desired summary statistics.

```{r}
head(diagnostics$numObservedEntities)
(posteriorMean <- mean(diagnostics$numObservedEntities))
(posteriorVariance <- var(diagnostics$numObservedEntities))
posteriorStd <- sqrt(posteriorVariance)
error <- qnorm(0.975)*posteriorStd/sqrt(numRecords) 
(cbind(posteriorMean - error, posteriorMean + error))
```


Second, we may wish to visualize the estimated posterior distribution, while comparing this to the true
value (red) and the posterior mean (blue). Again, we can utilize the posterior samples, which are stored in diagnostics in order to generate a histogram as shown below. 

```{r posterior-plot}
ggplot(diagnostics) + 
  geom_histogram(aes(x=numObservedEntities), binwidth = 1) + 
  geom_vline(aes(xintercept = posteriorMean), color='blue') + 
  geom_vline(aes(xintercept = 450), color='red') + 
  labs(x = "# of entities", y = "Frequency", title = "Posterior number of observed entities")
```

In addition, we can place a credible interval (green) onto the resulting histogram quite easily, which we do below. 

```{r posterior-plot-credible}
ggplot(diagnostics) + 
  geom_histogram(aes(x=numObservedEntities), binwidth = 1) + 
  geom_vline(aes(xintercept = posteriorMean), color='blue') + 
  geom_vline(aes(xintercept = 450), color='red') + 
  geom_vline(aes(xintercept = posteriorMean - error), color='green') + 
  geom_vline(aes(xintercept = posteriorMean + error), color='green') + 
  labs(x = "# of entities", y = "Frequency", title = "Posterior number of observed entities")
```


Third, we consider the distortion level observed in each attribute, averaged 
across all of the records. This information is also stored in the 
diagnostics. Below we plot the posterior median for each attribute with a 
95\% highest density interval.

```{r posterior-attribute-distortion}
diagnostics %>%
  select(iteration, starts_with("aggDist")) %>%
  gather(attribute, numDistortions, starts_with("aggDist")) %>%
  transmute(attribute = str_match(attribute, "^aggDist(.+)")[,2],
            percDistorted = numDistortions / nrow(records)) %>% 
  group_by(attribute) %>%
  point_interval(.interval = hdi) %>%
  ggplot(aes(attribute, percDistorted)) + 
    geom_pointinterval(interval_size_range = c(0.1, 1), position=position_dodge(width = 0.9), fatten_point = 1.4) + 
    coord_flip() + 
    labs(x = "Attribute", y = "Distortion level (%)", title = "Posterior distortion level by attribute")
```
